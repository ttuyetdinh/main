{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import CubicSpline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_video = {\n",
    "    \"positions\": {\n",
    "        \"1.0\": {\n",
    "            \"Nose\": [1, 1, 1],\n",
    "            \"Left_eye\": [1, 1, 1],\n",
    "        },\n",
    "        \"2.0\": {\n",
    "            \"Nose\": [2, 2, 2],\n",
    "            \"Left_eye\": [2, 2, 2],\n",
    "        },\n",
    "        \"3.0\": {\n",
    "            \"Nose\": [3, 3, 3],\n",
    "            \"Left_eye\": [3, 3, 3],\n",
    "        },\n",
    "        \"4.0\": {\n",
    "            \"Nose\": [4, 4, 4],\n",
    "            \"Left_eye\": [4, 4, 4],\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "sample_video[\"positions\"][\"1.0\"][\"Nose\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <!-- for column in row.index:\n",
    "            # Extract joint name and position (e.g., Nose_x)\n",
    "            joint = '_'.join(column.split('_')[1:])\n",
    "            if joint not in mean_data_series:\n",
    "                mean_data_series[joint] = []\n",
    "            mean_data_series[joint].append(row[column]) -->\n",
    "\n",
    "Let say we have\n",
    "a[1_a_x:5, 1_a_y:4, 2_a_x:3, 2_a_y:7]\n",
    "The above code give:\n",
    "result = {\n",
    "a_x:[5,3],\n",
    "a_y:[4,7]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesPath(directory):\n",
    "    files = []\n",
    "    for root, dirs, file in os.walk(directory):\n",
    "        for f in file:\n",
    "            files.append(os.path.join(root, f))\n",
    "    return files\n",
    "\n",
    "\n",
    "def loadFiles(files_path, activities, joints_mapping, target_frames=30, max_workers=4):\n",
    "    \"\"\"\n",
    "    Load and process activity data files.\n",
    "\n",
    "    This function loads JSON files containing activity data, resamples the data to a target number of frames,\n",
    "    flattens the data, and combines it into a single DataFrame. The function filters files based on the specified\n",
    "    activities and processes only those files.\n",
    "\n",
    "    Parameters:\n",
    "    - files_path (list of str): List of file paths to be processed.\n",
    "    - activities (list of str): List of activity names to filter the files.\n",
    "    - joints_mapping (list): Dictionary mapping joint names to their indices.\n",
    "    - target_frames (int, optional): The target number of frames to resample the data to. Default is 30.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the processed and combined activity data. If no data is processed, \n",
    "    an empty DataFrame is returned.\n",
    "    \"\"\"\n",
    "    activity_dfs = []\n",
    "    for activity in activities:\n",
    "        for file_path in files_path:\n",
    "            if file_path.endswith('.json') and activity in file_path:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    file_data = json.load(file)\n",
    "                    resample_file_data = resample_video(file_data, joints_mapping, target_frames)\n",
    "                    if resample_file_data is None:\n",
    "                        return None\n",
    "                    file_df = flatternData(resample_file_data, joints_mapping)\n",
    "                    file_df.insert(0, 'Group', file_path.split('\\\\')[-1].split('-')[0])\n",
    "                    file_df.insert(1, 'Activity', activity)\n",
    "                    return file_df\n",
    "    if activity_dfs:\n",
    "        final_df = pd.concat(activity_dfs, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def flatternData(data, joints_mapping):\n",
    "    # Create a list of dictionaries for each timestamp and joint\n",
    "    flattened_data = [\n",
    "        {f'{timestamp}_{joint}_{axis}': position[joint][i]\n",
    "            for joint in joints_mapping if joint in position\n",
    "            for i, axis in enumerate(['x', 'y', 'z'])}\n",
    "        for timestamp, position in data.items()\n",
    "    ]\n",
    "\n",
    "    # Flatten the list of dictionaries into a single dictionary\n",
    "    flattened_data = {k: v for d in flattened_data for k, v in d.items()}\n",
    "    return pd.DataFrame([flattened_data])\n",
    "\n",
    "\n",
    "def resample_video(video_data, joint_mapping, target_frames=30):\n",
    "    # Get the original frame numbers and convert to float\n",
    "    original_frames = np.array([float(k) for k in video_data['positions'].keys()])\n",
    "    # check that video has at least 4 frames\n",
    "\n",
    "    if len(original_frames) < 4:\n",
    "        return None\n",
    "    # Create new evenly spaced frames\n",
    "    new_frames = np.linspace(min(original_frames), max(original_frames), target_frames)\n",
    "\n",
    "    # Get all unique joints\n",
    "    joints = list(next(iter(video_data['positions'].values())).keys())\n",
    "\n",
    "    # Initialize the resampled data structure\n",
    "    resampled_data = {str(float(i)): {} for i in range(1, target_frames + 1)}\n",
    "\n",
    "    # Interpolate each joint's coordinates\n",
    "    for joint in joint_mapping:\n",
    "        # Extract x, y, z coordinates for the current joint across all frames\n",
    "        coords = np.array([video_data['positions'][str(frame)][joint] for frame in original_frames])\n",
    "        x_coords, y_coords, z_coords = coords[:, 0], coords[:, 1], coords[:, 2]\n",
    "\n",
    "        # Create cubic interpolation functions for each coordinate\n",
    "        x_interp = interp1d(original_frames, x_coords, kind='cubic')\n",
    "        y_interp = interp1d(original_frames, y_coords, kind='cubic')\n",
    "        z_interp = interp1d(original_frames, z_coords, kind='cubic')\n",
    "\n",
    "        # Apply interpolation to get new coordinates\n",
    "        for i, frame in enumerate(new_frames, 1):  # Start counting from 1\n",
    "            resampled_data[str(float(i))][joint] = [\n",
    "                float(x_interp(frame)),\n",
    "                float(y_interp(frame)),\n",
    "                float(z_interp(frame))\n",
    "            ]\n",
    "    return resampled_data\n",
    "\n",
    "\n",
    "def calculateMeanForAllVideos(df):\n",
    "    # Store the group and activity columns\n",
    "    labed_df = df[['Group', 'Activity']]\n",
    "\n",
    "    # Ignore the group and activity columns for mean calculation\n",
    "    df = df.drop(columns=['Group', 'Activity'])\n",
    "\n",
    "    # Extract joint names and coordinates\n",
    "    joint_names = df.columns.str.split('_').str[1:].str.join('_')\n",
    "\n",
    "    # Calculate mean for each joint\n",
    "    mean_df = df.T.groupby(joint_names).mean().T\n",
    "\n",
    "    # Add back the group and activity columns\n",
    "    mean_df.insert(0, 'Group', labed_df['Group'])\n",
    "    mean_df.insert(1, 'Activity', labed_df['Activity'])\n",
    "\n",
    "    return mean_df\n",
    "\n",
    "\n",
    "def group_by_activity(dfs):\n",
    "    # Concatenate all DataFrames in the list\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    # Get unique activities\n",
    "    activities = merged_df['Activity'].unique()\n",
    "\n",
    "    # Split the merged DataFrame by activity\n",
    "    split_dfs = {activity: merged_df[merged_df['Activity'] == activity] for activity in activities}\n",
    "\n",
    "    return split_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of joint names in OpenPose\n",
    "openpose_joint_mapping = [\n",
    "    \"Nose\",\n",
    "    \"Left_eye\",\n",
    "    \"Right_eye\",\n",
    "    \"Left_ear\",\n",
    "    \"Right_ear\",\n",
    "    \"Left_shoulder\",\n",
    "    \"Right_shoulder\",\n",
    "    \"Left_elbow\",\n",
    "    \"Right_elbow\",\n",
    "    \"Left_wrist\",\n",
    "    \"Right_wrist\",\n",
    "    \"Left_hip\",\n",
    "    \"Right_hip\",\n",
    "    \"Left_knee\",\n",
    "    \"Right_knee\",\n",
    "    \"Left_ankle\",\n",
    "    \"Right_ankle\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G2A-BP-CTK-S1-Brest-029.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-CTK-S1-Brest-034.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-CTK-S1-Brest-090.json on ThreadPoolExecutor-9_2\n",
      "G2A-BP-CTK-S1-Brest-093.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-CTK-S1-Brest-096.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-CTK-S1-Roscoff-009.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-CTK-S1-Roscoff-012.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-CTK-S1-Roscoff-013.json on ThreadPoolExecutor-9_2\n",
      "G2A-BP-CTK-S1-Roscoff-043.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-CTK-S1-Roscoff-051.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-CTK-S1-Roscoff-059.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-CTK-S2-Roscoff-003.json on ThreadPoolExecutor-9_2\n",
      "G2A-BP-CTK-S2-Roscoff-018.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-CTK-S2-Roscoff-021.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-CTK-S2-Roscoff-025.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-CTK-S3-Roscoff-077.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-CTK-S3-Roscoff-091.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-CTK-S4-Roscoff-001.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-CTK-S4-Roscoff-015.json on ThreadPoolExecutor-9_2\n",
      "G2A-BP-CTK-S4-Roscoff-026.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-CTK-S4-Roscoff-046.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-CTK-S4-Roscoff-068.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-CTK-S4-Roscoff-074.json on ThreadPoolExecutor-9_2\n",
      "G2A-BP-CTK-S4-Roscoff-088.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-ELK-S1-Roscoff-005.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-ELK-S1-Roscoff-060.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-ELK-S1-Roscoff-071.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-ELK-S1-Roscoff-072.json on ThreadPoolExecutor-9_2\n",
      "G2A-BP-ELK-S1-Roscoff-077.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-ELK-S3-Roscoff-007.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-ELK-S3-Roscoff-021.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-ELK-S3-Roscoff-085.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-RTK-S1-Roscoff-005.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-RTK-S1-Roscoff-010.json on ThreadPoolExecutor-9_2\n",
      "G2A-BP-RTK-S1-Roscoff-025.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-RTK-S1-Roscoff-049.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-RTK-S1-Roscoff-050.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-RTK-S1-Roscoff-052.json on ThreadPoolExecutor-9_2\n",
      "G2A-BP-RTK-S1-Roscoff-072.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-RTK-S2-Roscoff-021.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-RTK-S2-Roscoff-063.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-RTK-S3-Roscoff-030.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-RTK-S3-Roscoff-084.json on ThreadPoolExecutor-9_2\n",
      "G2A-BP-RTK-S3-Roscoff-095.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-RTK-S5-Brest-026.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-RTK-S5-Brest-043.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-RTK-S5-Brest-074.json on ThreadPoolExecutor-9_2\n",
      "G2A-BP-RTK-S6-Roscoff-042.json on ThreadPoolExecutor-9_0\n",
      "G2A-BP-RTK-S6-Roscoff-062.json on ThreadPoolExecutor-9_3\n",
      "G2A-BP-RTK-S6-Roscoff-071.json on ThreadPoolExecutor-9_1\n",
      "G2A-BP-RTK-S6-Roscoff-077.json on ThreadPoolExecutor-9_3\n"
     ]
    }
   ],
   "source": [
    "# path to data\n",
    "root_path = os.getcwd()\n",
    "group1A_path = os.path.join(root_path, 'Group1A', 'blazepose')\n",
    "group2A_path = os.path.join(root_path, 'Group2A', 'blazepose')\n",
    "group3_path = os.path.join(root_path, 'Group3', 'blazepose')\n",
    "\n",
    "# group1B_path = os.path.join(root_path, 'Group1B', 'blazepose')\n",
    "group2B_path = os.path.join(root_path, 'Group2B', 'blazepose')\n",
    "\n",
    "# activities\n",
    "activities = ['CTK', 'ELK', 'RTK']\n",
    "# activities = ['ELK']\n",
    "\n",
    "# Get paths\n",
    "group1A_files_path = getFilesPath(group1A_path)\n",
    "group2A_files_path = getFilesPath(group2A_path)\n",
    "group3_files_path = getFilesPath(group3_path)\n",
    "\n",
    "# group1B_files_path = getFilesPath(group1B_path)\n",
    "group2B_files_path = getFilesPath(group2B_path)\n",
    "\n",
    "# Load data\n",
    "# group1A_df = loadFiles(group1A_files_path, activities, openpose_joint_mapping, 200)\n",
    "group2A_df = loadFiles(group2A_files_path, activities, openpose_joint_mapping, 200, 4)\n",
    "# group3_df = loadFiles(group3_files_path, activities, openpose_joint_mapping, 200)\n",
    "\n",
    "# Store data to csv\n",
    "# group1A_df.to_csv('group1A.csv', index=False)\n",
    "group2A_df.to_csv('group2A.csv', index=False)\n",
    "# group3_df.to_csv('group3.csv', index=False)\n",
    "\n",
    "# group1B_df = loadFiles(group1B_files_path, activities,openpose_joint_mapping )\n",
    "# group2B_df = loadFiles(group2B_files_path, activities,openpose_joint_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'group1A.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data from csv\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m group1A_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup1A.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m group2A_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup2A.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m group3_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup3.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\bp_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\bp_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\bp_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\bp_env\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\anaconda3\\envs\\bp_env\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'group1A.csv'"
     ]
    }
   ],
   "source": [
    "# Load data from csv\n",
    "group1A_df = pd.read_csv('group1A.csv')\n",
    "group2A_df = pd.read_csv('group2A.csv')\n",
    "group3_df = pd.read_csv('group3.csv')\n",
    "# group1B_df = pd.read_csv('group1B.csv')\n",
    "# group2B_df = pd.read_csv('group2B.csv')\n",
    "group2A_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Activity</th>\n",
       "      <th>Backpain</th>\n",
       "      <th>Left_ankle_x</th>\n",
       "      <th>Left_ankle_y</th>\n",
       "      <th>Left_ankle_z</th>\n",
       "      <th>Left_ear_x</th>\n",
       "      <th>Left_ear_y</th>\n",
       "      <th>Left_ear_z</th>\n",
       "      <th>Left_elbow_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Right_hip_z</th>\n",
       "      <th>Right_knee_x</th>\n",
       "      <th>Right_knee_y</th>\n",
       "      <th>Right_knee_z</th>\n",
       "      <th>Right_shoulder_x</th>\n",
       "      <th>Right_shoulder_y</th>\n",
       "      <th>Right_shoulder_z</th>\n",
       "      <th>Right_wrist_x</th>\n",
       "      <th>Right_wrist_y</th>\n",
       "      <th>Right_wrist_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G1A</td>\n",
       "      <td>CTK</td>\n",
       "      <td>1</td>\n",
       "      <td>0.470761</td>\n",
       "      <td>0.758881</td>\n",
       "      <td>0.037198</td>\n",
       "      <td>0.469344</td>\n",
       "      <td>0.370807</td>\n",
       "      <td>-0.036697</td>\n",
       "      <td>0.457347</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>0.476370</td>\n",
       "      <td>0.662420</td>\n",
       "      <td>0.017133</td>\n",
       "      <td>0.474796</td>\n",
       "      <td>0.424234</td>\n",
       "      <td>-0.017197</td>\n",
       "      <td>0.489964</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>-0.027319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G1A</td>\n",
       "      <td>CTK</td>\n",
       "      <td>1</td>\n",
       "      <td>0.498472</td>\n",
       "      <td>0.794799</td>\n",
       "      <td>0.053960</td>\n",
       "      <td>0.505062</td>\n",
       "      <td>0.423639</td>\n",
       "      <td>-0.022104</td>\n",
       "      <td>0.485954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008531</td>\n",
       "      <td>0.514236</td>\n",
       "      <td>0.704524</td>\n",
       "      <td>0.005048</td>\n",
       "      <td>0.517393</td>\n",
       "      <td>0.474906</td>\n",
       "      <td>-0.037333</td>\n",
       "      <td>0.531627</td>\n",
       "      <td>0.480544</td>\n",
       "      <td>-0.046474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Group Activity  Backpain  Left_ankle_x  Left_ankle_y  Left_ankle_z  \\\n",
       "0   G1A      CTK         1      0.470761      0.758881      0.037198   \n",
       "1   G1A      CTK         1      0.498472      0.794799      0.053960   \n",
       "\n",
       "   Left_ear_x  Left_ear_y  Left_ear_z  Left_elbow_x  ...  Right_hip_z  \\\n",
       "0    0.469344    0.370807   -0.036697      0.457347  ...     0.004945   \n",
       "1    0.505062    0.423639   -0.022104      0.485954  ...    -0.008531   \n",
       "\n",
       "   Right_knee_x  Right_knee_y  Right_knee_z  Right_shoulder_x  \\\n",
       "0      0.476370      0.662420      0.017133          0.474796   \n",
       "1      0.514236      0.704524      0.005048          0.517393   \n",
       "\n",
       "   Right_shoulder_y  Right_shoulder_z  Right_wrist_x  Right_wrist_y  \\\n",
       "0          0.424234         -0.017197       0.489964       0.433962   \n",
       "1          0.474906         -0.037333       0.531627       0.480544   \n",
       "\n",
       "   Right_wrist_z  \n",
       "0      -0.027319  \n",
       "1      -0.046474  \n",
       "\n",
       "[2 rows x 54 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate mean for all videos\n",
    "group1A_mean_df = calculateMeanForAllVideos(group1A_df)\n",
    "group2A_mean_df = calculateMeanForAllVideos(group2A_df)\n",
    "group3_mean_df = calculateMeanForAllVideos(group3_df)\n",
    "\n",
    "# Add labels\n",
    "group1A_mean_df.insert(2, 'Backpain', 1)\n",
    "group2A_mean_df.insert(2, 'Backpain', 0)\n",
    "group3_mean_df.insert(2, 'Backpain', 0)\n",
    "\n",
    "grouped_activities = group_by_activity([group1A_mean_df, group2A_mean_df, group3_mean_df])\n",
    "\n",
    "grouped_activities['CTK'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Best parameters: {'algorithm': 'auto', 'n_neighbors': 3, 'weights': 'distance'}\n",
      "Best cross-validation score: 0.9824154589371981\n",
      "Test set accuracy: 0.9827586206896551\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming merged_df_by_activity is already defined and contains the merged DataFrame for each activity\n",
    "df_ctk = grouped_activities['CTK']\n",
    "\n",
    "# Prepare the data\n",
    "X = df_ctk.drop(columns=['Group', 'Activity', 'Backpain'])  # Features\n",
    "y = df_ctk['Backpain']  # Labels\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for the KNeighborsClassifier\n",
    "param_grid = {\n",
    "    'n_neighbors': [3, 4, 5],\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "\n",
    "# Initialize the KNeighborsClassifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn_clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=3)\n",
    "\n",
    "# Perform grid search and cross-validation\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_}\")\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = grid_search.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test set accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "group2B_df = loadFiles(group2B_files_path, activities, openpose_joint_mapping)\n",
    "\n",
    "group2B_mean_df = calculateMeanForAllVideos(group2B_df)\n",
    "group2B_mean_df.insert(2, 'Backpain', 0)\n",
    "\n",
    "grouped2B_activities = group_by_activity([group2B_mean_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 2B test set accuracy: 0.40350877192982454\n"
     ]
    }
   ],
   "source": [
    "# use the above model to predict the backpain for group2B\n",
    "df_2B = grouped2B_activities['CTK']\n",
    "X_2B = df_2B.drop(columns=['Group', 'Activity', 'Backpain'])\n",
    "y_2B = df_2B['Backpain']\n",
    "\n",
    "y_pred_2B = grid_search.predict(X_2B)\n",
    "accuracy_2B = accuracy_score(y_2B, y_pred_2B)\n",
    "\n",
    "print(f\"Group 2B test set accuracy: {accuracy_2B}\")\n",
    "# print(group2B_mean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.0_Nose_x</th>\n",
       "      <th>1.0_Nose_y</th>\n",
       "      <th>1.0_Nose_z</th>\n",
       "      <th>1.0_Left_eye_x</th>\n",
       "      <th>1.0_Left_eye_y</th>\n",
       "      <th>1.0_Left_eye_z</th>\n",
       "      <th>2.0_Nose_x</th>\n",
       "      <th>2.0_Nose_y</th>\n",
       "      <th>2.0_Nose_z</th>\n",
       "      <th>2.0_Left_eye_x</th>\n",
       "      <th>...</th>\n",
       "      <th>3.0_Nose_z</th>\n",
       "      <th>3.0_Left_eye_x</th>\n",
       "      <th>3.0_Left_eye_y</th>\n",
       "      <th>3.0_Left_eye_z</th>\n",
       "      <th>4.0_Nose_x</th>\n",
       "      <th>4.0_Nose_y</th>\n",
       "      <th>4.0_Nose_z</th>\n",
       "      <th>4.0_Left_eye_x</th>\n",
       "      <th>4.0_Left_eye_y</th>\n",
       "      <th>4.0_Left_eye_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1.0_Nose_x  1.0_Nose_y  1.0_Nose_z  1.0_Left_eye_x  1.0_Left_eye_y  \\\n",
       "0           1           1           1               1               1   \n",
       "\n",
       "   1.0_Left_eye_z  2.0_Nose_x  2.0_Nose_y  2.0_Nose_z  2.0_Left_eye_x  ...  \\\n",
       "0               1           2           2           2               2  ...   \n",
       "\n",
       "   3.0_Nose_z  3.0_Left_eye_x  3.0_Left_eye_y  3.0_Left_eye_z  4.0_Nose_x  \\\n",
       "0           3               3               3               3           4   \n",
       "\n",
       "   4.0_Nose_y  4.0_Nose_z  4.0_Left_eye_x  4.0_Left_eye_y  4.0_Left_eye_z  \n",
       "0           4           4               4               4               4  \n",
       "\n",
       "[1 rows x 24 columns]"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = flatternData(sample_video['positions'], openpose_joint_mapping)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateStatisticalFeatures(df):\n",
    "    features = {}\n",
    "    for column in df.columns:\n",
    "        features[f'{column}_mean'] = df[column].mean()\n",
    "        features[f'{column}_std'] = df[column].std()\n",
    "        features[f'{column}_min'] = df[column].min()\n",
    "        features[f'{column}_max'] = df[column].max()\n",
    "    return features\n",
    "\n",
    "\n",
    "def labelData(dataframes, label):\n",
    "    labeled_data = []\n",
    "    for df in dataframes:\n",
    "        features = calculateStatisticalFeatures(df)\n",
    "        features['label'] = label\n",
    "        labeled_data.append(features)\n",
    "    return labeled_data\n",
    "\n",
    "\n",
    "def combineData(labeled_data):\n",
    "    return pd.DataFrame(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[168], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m group2A_labeled_data \u001b[38;5;241m=\u001b[39m labelData(group2A_df, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[167], line 14\u001b[0m, in \u001b[0;36mlabelData\u001b[1;34m(dataframes, label)\u001b[0m\n\u001b[0;32m     12\u001b[0m labeled_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m df \u001b[38;5;129;01min\u001b[39;00m dataframes:\n\u001b[1;32m---> 14\u001b[0m     features \u001b[38;5;241m=\u001b[39m calculateStatisticalFeatures(df)\n\u001b[0;32m     15\u001b[0m     features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m label\n\u001b[0;32m     16\u001b[0m     labeled_data\u001b[38;5;241m.\u001b[39mappend(features)\n",
      "Cell \u001b[1;32mIn[167], line 3\u001b[0m, in \u001b[0;36mcalculateStatisticalFeatures\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculateStatisticalFeatures\u001b[39m(df):\n\u001b[0;32m      2\u001b[0m     features \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m column \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m      4\u001b[0m         features[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m      5\u001b[0m         features[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_std\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[column]\u001b[38;5;241m.\u001b[39mstd()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "group2A_labeled_data = labelData(group2A_df, label=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
