{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.interpolate import CubicSpline\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_video = {\n",
    "    \"positions\": {\n",
    "        \"1.0\": {\n",
    "            \"Nose\": [1, 1, 1],\n",
    "            \"Left_eye\": [1, 1, 1],\n",
    "        },\n",
    "        \"2.0\": {\n",
    "            \"Nose\": [2, 2, 2],\n",
    "            \"Left_eye\": [2, 2, 2],\n",
    "        },\n",
    "        \"3.0\": {\n",
    "            \"Nose\": [3, 3, 3],\n",
    "            \"Left_eye\": [3, 3, 3],\n",
    "        },\n",
    "        \"4.0\": {\n",
    "            \"Nose\": [4, 4, 4],\n",
    "            \"Left_eye\": [4, 4, 4],\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "sample_video[\"positions\"][\"1.0\"][\"Nose\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <!-- for column in row.index:\n",
    "            # Extract joint name and position (e.g., Nose_x)\n",
    "            joint = '_'.join(column.split('_')[1:])\n",
    "            if joint not in mean_data_series:\n",
    "                mean_data_series[joint] = []\n",
    "            mean_data_series[joint].append(row[column]) -->\n",
    "\n",
    "Let say we have\n",
    "a[1_a_x:5, 1_a_y:4, 2_a_x:3, 2_a_y:7]\n",
    "The above code give:\n",
    "result = {\n",
    "a_x:[5,3],\n",
    "a_y:[4,7]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFilesPath(directory):\n",
    "    files = []\n",
    "    for root, dirs, file in os.walk(directory):\n",
    "        for f in file:\n",
    "            files.append(os.path.join(root, f))\n",
    "    return files\n",
    "\n",
    "\n",
    "def loadFiles(files_path, activities, joints_mapping, target_frames=30):\n",
    "    \"\"\"\n",
    "    Load and process activity data files.\n",
    "\n",
    "    This function loads JSON files containing activity data, resamples the data to a target number of frames,\n",
    "    flattens the data, and combines it into a single DataFrame. The function filters files based on the specified\n",
    "    activities and processes only those files.\n",
    "\n",
    "    Parameters:\n",
    "    - files_path (list of str): List of file paths to be processed.\n",
    "    - activities (list of str): List of activity names to filter the files.\n",
    "    - joints_mapping (list): Dictionary mapping joint names to their indices.\n",
    "    - target_frames (int, optional): The target number of frames to resample the data to. Default is 30.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing the processed and combined activity data. If no data is processed, \n",
    "    an empty DataFrame is returned.\n",
    "    \"\"\"\n",
    "    activity_dfs = []\n",
    "    for activity in activities:\n",
    "        for file_path in files_path:\n",
    "            if file_path.endswith('.json') and activity in file_path:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    file_data = json.load(file)\n",
    "                    resample_file_data = resample_video(file_data, joints_mapping, target_frames)\n",
    "                    if resample_file_data is None:\n",
    "                        continue\n",
    "                    file_df = flatternData(resample_file_data, joints_mapping)\n",
    "                    file_df.insert(0, 'Group', file_path.split('\\\\')[-1].split('-')[0])\n",
    "                    file_df.insert(1, 'Activity', activity)\n",
    "                    activity_dfs.append(file_df)\n",
    "\n",
    "    if activity_dfs:\n",
    "        final_df = pd.concat(activity_dfs, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def flatternData(data, joints_mapping):\n",
    "    # Create a list of dictionaries for each timestamp and joint\n",
    "    flattened_data = [\n",
    "        {f'{timestamp}_{joint}_{axis}': position[joint][i]\n",
    "            for joint in joints_mapping if joint in position\n",
    "            for i, axis in enumerate(['x', 'y', 'z'])}\n",
    "        for timestamp, position in data.items()\n",
    "    ]\n",
    "\n",
    "    # Flatten the list of dictionaries into a single dictionary\n",
    "    flattened_data = {k: v for d in flattened_data for k, v in d.items()}\n",
    "    return pd.DataFrame([flattened_data])\n",
    "\n",
    "\n",
    "def resample_video(video_data, joint_mapping, target_frames=30):\n",
    "    # Get the original frame numbers and convert to float\n",
    "    original_frames = np.array([float(k) for k in video_data['positions'].keys()])\n",
    "    # check that video has at least 4 frames\n",
    "\n",
    "    if len(original_frames) < 50:\n",
    "        return None\n",
    "    # Create new evenly spaced frames\n",
    "    new_frames = np.linspace(min(original_frames), max(original_frames), target_frames)\n",
    "\n",
    "    # Get all unique joints\n",
    "    joints = list(next(iter(video_data['positions'].values())).keys())\n",
    "\n",
    "    # Initialize the resampled data structure\n",
    "    resampled_data = {str(float(i)): {} for i in range(1, target_frames + 1)}\n",
    "\n",
    "    # Interpolate each joint's coordinates\n",
    "    for joint in joint_mapping:\n",
    "        # Extract x, y, z coordinates for the current joint across all frames\n",
    "        coords = np.array([video_data['positions'][str(frame)][joint] for frame in original_frames])\n",
    "        x_coords, y_coords, z_coords = coords[:, 0], coords[:, 1], coords[:, 2]\n",
    "\n",
    "        # Create cubic interpolation functions for each coordinate\n",
    "        x_interp = interp1d(original_frames, x_coords, kind='cubic')\n",
    "        y_interp = interp1d(original_frames, y_coords, kind='cubic')\n",
    "        z_interp = interp1d(original_frames, z_coords, kind='cubic')\n",
    "\n",
    "        # Apply interpolation to get new coordinates\n",
    "        for i, frame in enumerate(new_frames, 1):  # Start counting from 1\n",
    "            resampled_data[str(float(i))][joint] = [\n",
    "                float(x_interp(frame)),\n",
    "                float(y_interp(frame)),\n",
    "                float(z_interp(frame))\n",
    "            ]\n",
    "    return resampled_data\n",
    "\n",
    "\n",
    "def calculateFeatureForAllVideos(df):\n",
    "    # Store the group and activity columns\n",
    "    labed_df = df[['Group', 'Activity']]\n",
    "\n",
    "    # Ignore the group and activity columns for calculation\n",
    "    df = df.drop(columns=['Group', 'Activity'])\n",
    "\n",
    "    # Extract joint names and coordinates\n",
    "    joint_names = df.columns.str.split('_').str[1:].str.join('_')\n",
    "\n",
    "    # Calculate statistics for each joint\n",
    "    mean_df = df.T.groupby(joint_names).mean().T\n",
    "    median_df = df.T.groupby(joint_names).median().T\n",
    "    std_df = df.T.groupby(joint_names).std().T\n",
    "    var_df = df.T.groupby(joint_names).var().T\n",
    "    min_df = df.T.groupby(joint_names).min().T\n",
    "    max_df = df.T.groupby(joint_names).max().T\n",
    "    range_df = max_df - min_df\n",
    "\n",
    "    # Combine all statistics into a single DataFrame\n",
    "    stats_df = pd.concat([mean_df, median_df, std_df, var_df, min_df, max_df, range_df], axis=1)\n",
    "    stats_df.columns = [f'{col}_mean' for col in mean_df.columns] + \\\n",
    "                       [f'{col}_median' for col in median_df.columns] + \\\n",
    "                       [f'{col}_std' for col in std_df.columns] + \\\n",
    "                       [f'{col}_var' for col in var_df.columns] + \\\n",
    "                       [f'{col}_min' for col in min_df.columns] + \\\n",
    "                       [f'{col}_max' for col in max_df.columns] + \\\n",
    "                       [f'{col}_range' for col in range_df.columns]\n",
    "\n",
    "    # Add back the group and activity columns\n",
    "    stats_df.insert(0, 'Group', labed_df['Group'])\n",
    "    stats_df.insert(1, 'Activity', labed_df['Activity'])\n",
    "\n",
    "    return stats_df\n",
    "\n",
    "\n",
    "def group_by_activity(dfs, onehot_columns):\n",
    "    # Concatenate all DataFrames in the list\n",
    "    merged_df = pd.concat(dfs, ignore_index=True)\n",
    "    # Onehot the onehot_columns\n",
    "    merged_df = pd.get_dummies(merged_df, columns=onehot_columns)\n",
    "    # Get unique activities\n",
    "    # activities = merged_df['Activity'].unique()\n",
    "\n",
    "    # # Split the merged DataFrame by activity\n",
    "    # split_dfs = {activity: merged_df[merged_df['Activity'] == activity] for activity in activities}\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of joint names in OpenPose\n",
    "openpose_joint_mapping = [\n",
    "    \"Nose\",\n",
    "    \"Left_eye\",\n",
    "    \"Right_eye\",\n",
    "    \"Left_ear\",\n",
    "    \"Right_ear\",\n",
    "    \"Left_shoulder\",\n",
    "    \"Right_shoulder\",\n",
    "    \"Left_elbow\",\n",
    "    \"Right_elbow\",\n",
    "    \"Left_wrist\",\n",
    "    \"Right_wrist\",\n",
    "    \"Left_hip\",\n",
    "    \"Right_hip\",\n",
    "    \"Left_knee\",\n",
    "    \"Right_knee\",\n",
    "    \"Left_ankle\",\n",
    "    \"Right_ankle\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to data\n",
    "root_path = os.getcwd()\n",
    "group1A_path = os.path.join(root_path, 'Group1A', 'blazepose')\n",
    "group2A_path = os.path.join(root_path, 'Group2A', 'blazepose')\n",
    "group3_path = os.path.join(root_path, 'Group3', 'blazepose')\n",
    "\n",
    "group1B_path = os.path.join(root_path, 'Group1B', 'blazepose')\n",
    "group2B_path = os.path.join(root_path, 'Group2B', 'blazepose')\n",
    "\n",
    "# activities\n",
    "activities = ['CTK', 'ELK', 'RTK']\n",
    "# activities = ['ELK']\n",
    "\n",
    "# Get paths\n",
    "group1A_files_path = getFilesPath(group1A_path)\n",
    "group2A_files_path = getFilesPath(group2A_path)\n",
    "group3_files_path = getFilesPath(group3_path)\n",
    "\n",
    "group1B_files_path = getFilesPath(group1B_path)\n",
    "group2B_files_path = getFilesPath(group2B_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "group1A_df = loadFiles(group1A_files_path, activities, openpose_joint_mapping, 200)\n",
    "group2A_df = loadFiles(group2A_files_path, activities, openpose_joint_mapping, 200)\n",
    "group3_df = loadFiles(group3_files_path, activities, openpose_joint_mapping, 200)\n",
    "\n",
    "group1B_df = loadFiles(group1B_files_path, activities, openpose_joint_mapping, 200)\n",
    "group2B_df = loadFiles(group2B_files_path, activities, openpose_joint_mapping, 200)\n",
    "\n",
    "# Store data to csv\n",
    "group1A_df.to_csv('group1A.csv', index=False)\n",
    "group2A_df.to_csv('group2A.csv', index=False)\n",
    "group3_df.to_csv('group3.csv', index=False)\n",
    "\n",
    "group1B_df.to_csv('group1B.csv', index=False)\n",
    "group2B_df.to_csv('group2B.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>Activity</th>\n",
       "      <th>1.0_Nose_x</th>\n",
       "      <th>1.0_Nose_y</th>\n",
       "      <th>1.0_Nose_z</th>\n",
       "      <th>1.0_Left_eye_x</th>\n",
       "      <th>1.0_Left_eye_y</th>\n",
       "      <th>1.0_Left_eye_z</th>\n",
       "      <th>1.0_Right_eye_x</th>\n",
       "      <th>1.0_Right_eye_y</th>\n",
       "      <th>...</th>\n",
       "      <th>200.0_Left_knee_z</th>\n",
       "      <th>200.0_Right_knee_x</th>\n",
       "      <th>200.0_Right_knee_y</th>\n",
       "      <th>200.0_Right_knee_z</th>\n",
       "      <th>200.0_Left_ankle_x</th>\n",
       "      <th>200.0_Left_ankle_y</th>\n",
       "      <th>200.0_Left_ankle_z</th>\n",
       "      <th>200.0_Right_ankle_x</th>\n",
       "      <th>200.0_Right_ankle_y</th>\n",
       "      <th>200.0_Right_ankle_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G2A</td>\n",
       "      <td>CTK</td>\n",
       "      <td>0.539282</td>\n",
       "      <td>0.431655</td>\n",
       "      <td>-0.384252</td>\n",
       "      <td>0.543288</td>\n",
       "      <td>0.422329</td>\n",
       "      <td>-0.375004</td>\n",
       "      <td>0.535076</td>\n",
       "      <td>0.421193</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014480</td>\n",
       "      <td>0.493701</td>\n",
       "      <td>0.693743</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>0.566503</td>\n",
       "      <td>0.806848</td>\n",
       "      <td>0.067230</td>\n",
       "      <td>0.491475</td>\n",
       "      <td>0.797624</td>\n",
       "      <td>0.094346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G2A</td>\n",
       "      <td>CTK</td>\n",
       "      <td>0.536172</td>\n",
       "      <td>0.423099</td>\n",
       "      <td>-0.371713</td>\n",
       "      <td>0.541568</td>\n",
       "      <td>0.413143</td>\n",
       "      <td>-0.360224</td>\n",
       "      <td>0.530744</td>\n",
       "      <td>0.413438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044134</td>\n",
       "      <td>0.491112</td>\n",
       "      <td>0.689791</td>\n",
       "      <td>0.004020</td>\n",
       "      <td>0.569369</td>\n",
       "      <td>0.802822</td>\n",
       "      <td>0.067648</td>\n",
       "      <td>0.490229</td>\n",
       "      <td>0.793349</td>\n",
       "      <td>0.112206</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 10202 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Group Activity  1.0_Nose_x  1.0_Nose_y  1.0_Nose_z  1.0_Left_eye_x  \\\n",
       "0   G2A      CTK    0.539282    0.431655   -0.384252        0.543288   \n",
       "1   G2A      CTK    0.536172    0.423099   -0.371713        0.541568   \n",
       "\n",
       "   1.0_Left_eye_y  1.0_Left_eye_z  1.0_Right_eye_x  1.0_Right_eye_y  ...  \\\n",
       "0        0.422329       -0.375004         0.535076         0.421193  ...   \n",
       "1        0.413143       -0.360224         0.530744         0.413438  ...   \n",
       "\n",
       "   200.0_Left_knee_z  200.0_Right_knee_x  200.0_Right_knee_y  \\\n",
       "0          -0.014480            0.493701            0.693743   \n",
       "1          -0.044134            0.491112            0.689791   \n",
       "\n",
       "   200.0_Right_knee_z  200.0_Left_ankle_x  200.0_Left_ankle_y  \\\n",
       "0            0.007707            0.566503            0.806848   \n",
       "1            0.004020            0.569369            0.802822   \n",
       "\n",
       "   200.0_Left_ankle_z  200.0_Right_ankle_x  200.0_Right_ankle_y  \\\n",
       "0            0.067230             0.491475             0.797624   \n",
       "1            0.067648             0.490229             0.793349   \n",
       "\n",
       "   200.0_Right_ankle_z  \n",
       "0             0.094346  \n",
       "1             0.112206  \n",
       "\n",
       "[2 rows x 10202 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data from csv\n",
    "group1A_df = pd.read_csv('group1A.csv')\n",
    "group2A_df = pd.read_csv('group2A.csv')\n",
    "group3_df = pd.read_csv('group3.csv')\n",
    "group1B_df = pd.read_csv('group1B.csv')\n",
    "group2B_df = pd.read_csv('group2B.csv')\n",
    "group2A_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean for all videos\n",
    "group1A_mean_df = calculateFeatureForAllVideos(group1A_df)\n",
    "\n",
    "group2A_mean_df = calculateFeatureForAllVideos(group2A_df)\n",
    "\n",
    "group3_mean_df = calculateFeatureForAllVideos(group3_df)\n",
    "\n",
    "group1B_mean_df = calculateFeatureForAllVideos(group1B_df)\n",
    "group2B_mean_df = calculateFeatureForAllVideos(group2B_df)\n",
    "\n",
    "\n",
    "# Add labels\n",
    "\n",
    "group1A_mean_df.insert(2, 'Backpain', 1)\n",
    "\n",
    "group2A_mean_df.insert(2, 'Backpain', 0)\n",
    "\n",
    "group3_mean_df.insert(2, 'Backpain', 0)\n",
    "\n",
    "group1B_mean_df.insert(2, 'Backpain', 1)\n",
    "group2B_mean_df.insert(2, 'Backpain', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seglearn as sgl\n",
    "\n",
    "\n",
    "def perform_grid_search(X, y, param_grid, model, test_size=0.2, random_state=42, cv=5, n_jobs=6, verbose=3):\n",
    "    \"\"\"\n",
    "    Perform grid search and cross-validation for the given model and parameter grid.\n",
    "\n",
    "    Parameters:\n",
    "    - X (pd.DataFrame): The input features.\n",
    "    - y (pd.Series): The target labels.\n",
    "    - param_grid (dict): The parameter grid for GridSearchCV.\n",
    "    - model (sklearn estimator): The model to be used in the pipeline.\n",
    "    - test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.2.\n",
    "    - random_state (int, optional): The random seed for splitting the data. Default is 42.\n",
    "    - cv (int, optional): The number of cross-validation folds. Default is 5.\n",
    "    - n_jobs (int, optional): The number of jobs to run in parallel. Default is -1 (use all processors).\n",
    "    - verbose (int, optional): The verbosity level. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "    - grid_search (GridSearchCV): The GridSearchCV object after fitting.\n",
    "    - best_params (dict): The best parameters found by GridSearchCV.\n",
    "    - best_estimator (Pipeline): The best estimator found by GridSearchCV.\n",
    "    - accuracy (float): The accuracy of the model on the test set.\n",
    "    \"\"\"\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    # Define the pipeline\n",
    "    clf = Pipeline([\n",
    "        # (\"features\", sgl.FeatureRep()),\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "\n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=clf, param_grid=param_grid, cv=cv, n_jobs=n_jobs, verbose=verbose)\n",
    "\n",
    "    # Perform grid search and cross-validation\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "KNN best parameters: {'model__algorithm': 'auto', 'model__n_neighbors': 3, 'model__weights': 'distance'}\n",
      "KNN best score: 0.9613014799283757\n"
     ]
    }
   ],
   "source": [
    "train_dataset = group_by_activity(\n",
    "    [group1B_mean_df, group2B_mean_df, group3_mean_df],\n",
    "    ['Activity'])\n",
    "\n",
    "train_dataset.head(2)\n",
    "\n",
    "# Prepare the data\n",
    "X = train_dataset.drop(columns=['Group', 'Backpain'])  # Features\n",
    "y = train_dataset['Backpain']  # Labels\n",
    "\n",
    "# Perfom grid search for knn\n",
    "param_grid_knn = {\n",
    "    'model__n_neighbors': [3, 4, 5, 6],\n",
    "    'model__weights': ['uniform', 'distance'],\n",
    "    'model__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "}\n",
    "grid_search_knn = perform_grid_search(X, y, param_grid_knn, KNeighborsClassifier())\n",
    "\n",
    "# Performe grid search for random forest\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [10, 20, 30, 40, 50],\n",
    "    'model__min_samples_split': [2, 5, 10],\n",
    "    'model__min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "# grid_search_rf = perform_grid_search(X, y, param_grid_rf, RandomForestClassifier())\n",
    "\n",
    "# print the results for knn and random forest\n",
    "print(\"KNN best parameters:\", grid_search_knn.best_params_)\n",
    "print(\"KNN best score:\", grid_search_knn.best_score_)\n",
    "# print(\"Random Forest best parameters:\", grid_search_rf.best_params_)\n",
    "# print(\"Random Forest best score:\", grid_search_rf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1B&2B test set accuracy: 0.9081632653061225\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.58      0.68        50\n",
      "           1       0.92      0.98      0.95       244\n",
      "\n",
      "    accuracy                           0.91       294\n",
      "   macro avg       0.87      0.78      0.81       294\n",
      "weighted avg       0.90      0.91      0.90       294\n",
      "\n",
      "[[ 29  21]\n",
      " [  6 238]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "to_predict_dataset = group_by_activity([group1A_mean_df, group2A_mean_df], ['Activity'])\n",
    "\n",
    "# use the above model to predict the backpain for group2B\n",
    "\n",
    "X_to_predict = to_predict_dataset.drop(columns=['Group', 'Backpain'])\n",
    "\n",
    "y_to_predict = to_predict_dataset['Backpain']\n",
    "\n",
    "\n",
    "y_pred = grid_search_knn.predict(X_to_predict)\n",
    "\n",
    "accuracy_pred = accuracy_score(y_to_predict, y_pred)\n",
    "\n",
    "\n",
    "print(f\"Group 1B&2B test set accuracy: {accuracy_pred}\")\n",
    "\n",
    "\n",
    "# i want to see other metrics like precision, recall, f1-score, confusion matrix\n",
    "print(classification_report(y_to_predict, y_pred))\n",
    "print(confusion_matrix(y_to_predict, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>1.0_Nose_x</th>\n",
       "      <th>1.0_Nose_y</th>\n",
       "      <th>1.0_Nose_z</th>\n",
       "      <th>1.0_Left_eye_x</th>\n",
       "      <th>1.0_Left_eye_y</th>\n",
       "      <th>1.0_Left_eye_z</th>\n",
       "      <th>2.0_Nose_x</th>\n",
       "      <th>2.0_Nose_y</th>\n",
       "      <th>2.0_Nose_z</th>\n",
       "      <th>...</th>\n",
       "      <th>3.0_Left_eye_y</th>\n",
       "      <th>3.0_Left_eye_z</th>\n",
       "      <th>4.0_Nose_x</th>\n",
       "      <th>4.0_Nose_y</th>\n",
       "      <th>4.0_Nose_z</th>\n",
       "      <th>4.0_Left_eye_x</th>\n",
       "      <th>4.0_Left_eye_y</th>\n",
       "      <th>4.0_Left_eye_z</th>\n",
       "      <th>Activity_CTK</th>\n",
       "      <th>Activity_RTK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gr1a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gr1a</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Group  1.0_Nose_x  1.0_Nose_y  1.0_Nose_z  1.0_Left_eye_x  1.0_Left_eye_y  \\\n",
       "0  gr1a           1           1           1               1               1   \n",
       "1  gr1a           1           1           1               1               1   \n",
       "\n",
       "   1.0_Left_eye_z  2.0_Nose_x  2.0_Nose_y  2.0_Nose_z  ...  3.0_Left_eye_y  \\\n",
       "0               1           2           2           2  ...               3   \n",
       "1               1           2           2           2  ...               3   \n",
       "\n",
       "   3.0_Left_eye_z  4.0_Nose_x  4.0_Nose_y  4.0_Nose_z  4.0_Left_eye_x  \\\n",
       "0               3           4           4           4               4   \n",
       "1               3           4           4           4               4   \n",
       "\n",
       "   4.0_Left_eye_y  4.0_Left_eye_z  Activity_CTK  Activity_RTK  \n",
       "0               4               4          True         False  \n",
       "1               4               4         False          True  \n",
       "\n",
       "[2 rows x 27 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import test\n",
    "\n",
    "\n",
    "a1 = flatternData(sample_video['positions'], openpose_joint_mapping)\n",
    "a1.insert(0, 'Group', \"gr1a\")\n",
    "a1.insert(1, 'Activity', \"CTK\")\n",
    "a2 = flatternData(sample_video['positions'], openpose_joint_mapping)\n",
    "a2.insert(0, 'Group', \"gr1a\")\n",
    "a2.insert(1, 'Activity', \"RTK\")\n",
    "\n",
    "testdf = pd.concat([a1, a2], ignore_index=True)\n",
    "df_encoded = pd.get_dummies(testdf, columns=['Activity'])\n",
    "df_encoded"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
